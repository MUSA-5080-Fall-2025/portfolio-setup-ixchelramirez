---
title: "Philadelphia Housing Model - Technical Appendix"
author: "Ixchel Ramirez"
contributors: ""
format: 
  html:
    code-fold: show
    toc: true
    toc-location: left
    theme: cosmo
execute:
  warning: false
  message: false
---

### Phase 1: Data Preparation (Technical Appendix)

**Load and clean Philadelphia sales data:**

```{r load and clean sales data}
'warnings=false'

library(sf)
library(tigris)
library(tidycensus)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(caret)
library(scales)

#load census key for later use
#census_api_key("42bf8a20a3df1def380f330cf7edad0dd5842ce6")

#save data in url 
url <- "https://phl.carto.com/api/v2/sql?filename=opa_properties_public&format=geojson&skipfields=cartodb_id&q=SELECT+*+FROM+opa_properties_public"

#suppress warnings for clarity and read data as spatial object
suppressWarnings({
 property_data <- st_read(url)
})

#clean data
parcel_data <- property_data%>%
  select(location, #load columns that could potentially be used as predictors 
         category_code_description, #maybe garage_spaces 
         number_of_bedrooms,
         number_of_bathrooms, 
         total_livable_area,
         year_built,
         exterior_condition,
         garage_spaces,
         sale_price,
         sale_date)%>%
  filter(category_code_description %in% 
  c("SINGLE FAMILY","MULTI FAMILY"))%>% #no apartments, sales price of building
  drop_na(number_of_bedrooms, #remove anomalies like houses with no rooms
          number_of_bathrooms,
          total_livable_area,
          sale_price,
          year_built) %>%
  filter(number_of_bedrooms>0, 
         number_of_bathrooms>0,
         total_livable_area>0, 
         sale_price>=10000, sale_price <=1000000)%>% #remove very low/high prices
  mutate(sale_year = str_remove(sale_date, "-.*"))%>%   
  filter(sale_year %in% c("2023","2024")) #isolate to 2023 and 2024

```

**Load Secondary data:**

- Census data (tidycensus):
- Spatial amenities (OpenDataPhilly)

```{r load and clean census and spatial data}

#load data about poverty(counts and total), bachelors(counts and total), and income 
census_data <- get_acs(
  geography = "tract",
  state = "PA",
  county = "Philadelphia",
  variables = c(
    median_income = "B19013_001",
    num_with_bach = "B15003_022",
    bachelors_total ="B15003_001",
    num_in_poverty = "B17001_002",
    poverty_total ="B17001_001"
  ),
  year = 2022,
  output = "wide"
)

#create percentage columns for bachelors and poverty 
philly_census <- census_data%>%
  mutate(
    percentage_bach = num_with_bachE / bachelors_totalE,
    percentage_pov =  num_in_povertyE / poverty_totalE
  )

#remove data errors or incomplete fields 
philly_census <- philly_census%>%
  drop_na(median_incomeE)%>%
  filter(median_incomeE>0)

philly_census

#spatial census data 
philadelphia_tracts <- tracts(
  state = "PA",
  county = "Philadelphia",
  cb = TRUE,
  year = 2022
)

#join census data and tract geometry to PARCEL data
parcel_data <- parcel_data %>%
  st_transform(st_crs(philadelphia_tracts))%>%
  st_join(philadelphia_tracts, join = st_within)%>%
  left_join(philly_census, by = "GEOID")

#load university data
university_data <- st_read("../data/Universities_Colleges.geojson")

#load 2024 crime incident data
crime_data <-st_read("../data/incidents_part1_part2.shp")

#removed crime incidents with no geometry 
crime_data <- crime_data %>%
  filter(!st_is_empty(geometry))

#load neighborhood data 
neighborhoods <- st_read("../data/philadelphia-neighborhoods.shp")

```
**Summary Table:**

```{r summary table}
#BEFORE CLEANING
# Census data before cleaning
before_census_dim <- dim(census_data)
before_census_summary <- summary(census_data)

#AFTER CLEANING
after_census_dim <- dim(philly_census)
after_census_summary <- summary(philly_census)

# Summary table comparing before/after
data.frame(
  Stage = c("Before Cleaning", "After Cleaning"),
  Rows = c(before_census_dim[1], after_census_dim[1]),
  Columns = c(before_census_dim[2], after_census_dim[2])
)


```
**Choice for Data Cleaning**

Before starting the analysis it is necessary to clean each of the datasets to remove anomalies like houses with no rooms. Although we were looking at residential properties we did not include apartment building as it assigned the sale price of each apartment as the sale price of the building so this was removed. Hence, we are left with homes which mean they will include rooms. We also removed residential locations with sale prices under 10000 or over 1000000 which are very low and high prices and isolated parcel data to data between 2023 and 2024. Census data cleaning involved removing data errors or incomplete fields and adding a column to reflect percentages. The Census data started off with 408 rows and ended with 383 after the cleaning choices.
Other data that is also included is university and college locations, crime locations and neighborhood. These are cleaned in a later portion and involve transforming the coordinate reference system so that everything corresponds before filtering and recategorizing data to help with interpretation 


**Cleaning Data Continued: Not Census or Parcel Data**

```{r}

#set crs for distance calculations
crime_proj <- st_transform(crime_data, 3365)
parcel_proj <- st_transform(parcel_data, 3365)
university_proj <- st_transform(university_data, 3365)
neighborhood_proj <-st_transform(neighborhoods, 3365)

#logged sale price for comparison and modeling 
parcel_data <- parcel_data%>%
  mutate(log_sale_price = log(sale_price))

parcel_data <- parcel_data%>%
  mutate(log_livable_area = log(total_livable_area))
```


Distance to nearest college - this is a feature engineering component but needs to be run here for map 4 in phase 2
```{r feature engineering:knn }
#Distance to nearest college
university_proj <- st_transform(university_data, 3365)


dist_matrix <- st_distance(parcel_proj, university_proj)


get_knn_distance <- function(dist_matrix, k) {
  apply(dist_matrix, 1, function(distances) {
    mean(as.numeric(sort(distances)[1:k]))
  })
}

parcel_data$college_nn1 <- get_knn_distance(dist_matrix, k = 1)
parcel_data$college_nn3 <- get_knn_distance(dist_matrix, k = 3)
parcel_data$college_nn5 <- get_knn_distance(dist_matrix, k = 5)

parcel_data %>%
  st_drop_geometry() %>%
  select(sale_price, college_nn1, college_nn3, college_nn5) %>%
  cor(use = "complete.obs") %>%
  as.data.frame() %>%
  select(sale_price)

```



### Phase 2: Exploratory Data Analysis

**Create at least 5 professional visualizations:**

1. Distribution of sale prices (histogram)
2. Geographic distribution (map)
3. Price vs. structural features (scatter plots)
4. Price vs. spatial features (scatter plots)
5. One creative visualization

**For appendix:** Include all visualizations with detailed interpretations
1. Distribution of sale prices
```{r histogram}

ggplot(parcel_data, aes(x = sale_price)) +
  geom_histogram(binwidth = 25000, fill = "#1f78b4", color = "white") +
  scale_x_continuous(labels = scales::dollar_format()) +
  labs(
    title = "Distribution of Philadelphia Sale Prices (2023-2024)",
    x = "Sale Price ($)",
    y = "Number of Properties"
  ) +
  theme_minimal()

```
The graph above shows the highest concentration of parcel sale prices is around" `$200,000`. The histogram is right-skewed which means most sale prices are concentrated on the left side between zero and `$375,000`. In such a cases the mean is generally greater than the median because the tail which refers to the large values on the right could inflate the mean.   

2. Geographic distribution (map)
```{r histogram}

library(tmap)

tmap_mode("view")  # interactive map; use "plot" for static

tm_shape(parcel_data) +
  tm_dots(
    col = "sale_price",
    palette = "YlOrRd",
    style = "quantile",
    title = "Sale Price ($)",
    size = 0.05,
    alpha = 0.7
  ) +
  tm_basemap("OpenStreetMap") +
  tm_layout(title = "Geographic Distribution of Sale Prices in Philadelphia")


```
The geographic distribution of sale prices in Philadelphia shows that the most expensive sales were around Center City, North West Philadelphia and North East Philadelphia Airport.The lowest sales are in South and North Philadelphia. Some areas seem to increase from the lowest sale price to the highest without a transition of sale prices.


3. Price vs. structural features (scatter plots)
```{r histogram}

# Sale Price vs Total Livable Area
ggplot(parcel_data, aes(x = total_livable_area, y = sale_price)) +
  geom_point(alpha = 0.5, color = "#33a02c") +
  scale_y_continuous(labels = scales::dollar_format()) +
  labs(
    title = "Sale Price vs Total Livable Area",
    x = "Total Livable Area (sq ft)",
    y = "Sale Price ($)"
  ) +
  theme_minimal() +
  geom_smooth(method = "lm", col = "darkgreen")
  
# Sale Price vs Number of Bathrooms
ggplot(parcel_data, aes(x = number_of_bathrooms, y = sale_price)) +
  geom_jitter(alpha = 0.5, width = 0.2, color = "darkgreen") +
  scale_y_continuous(labels = scales::dollar_format()) +
  labs(
    title = "Sale Price vs Number of Bathrooms",
    x = "Number of Bathrooms",
    y = "Sale Price ($)"
  ) +
  theme_minimal()


# Sale Price vs Number of Bedrooms
ggplot(parcel_data, aes(x = number_of_bedrooms, y = sale_price)) +
  geom_jitter(alpha = 0.5, width = 0.2, color = "darkgreen") +
  scale_y_continuous(labels = scales::dollar_format()) +
  labs(
    title = "Sale Price vs Number of Bedrooms",
    x = "Number of Bedrooms",
    y = "Sale Price ($)"
  ) +
  theme_minimal()


```
Most places have a total livable area under 3000 sq ft, by 4000 sq ft the number is very sparce. Since the maximum for sale price is capped at `$1,000,000` there is a hard stop at this point on the y-axis. The most common type of sale was between zero to `$500,000` for 0 to 2000 total livable square feet.

The parcel that is sold the most contains 2 bathrooms and sales for under `$750,000`.


With respect to bathrooms the parcel that is sold the most contains 2 bathrooms and sales for under `$812,500`.

4. Price vs. spatial features (scatter plots)
```{r scatter plots}

# Scatter plot: Sale Price vs Distance to Nearest College
ggplot(parcel_data, aes(x = college_nn1, y = sale_price)) +
  geom_point(alpha = 0.5, color = "#6a3d9a") +
  geom_smooth(method = "lm", color = "blue", se = TRUE) +
  scale_y_continuous(labels = dollar_format(prefix = "$")) +
  scale_x_continuous(labels = comma_format(), limits = c(0, NA)) +
  labs(
    title = "Sale Price vs Distance to Nearest College",
    subtitle = "Distance calculated to the nearest university in feet",
    x = "Distance to Nearest College (ft)",
    y = "Sale Price ($)"
  ) +
  theme_minimal(base_size = 14)




```
The Sale Price vs Distance to Nearest College Map shows that as parcels are further from colleges their sale price decreases slightly due to the slight negative slope of the line. 


5. Creative visualization - Heatmap of Bedrooms in Philadelphia 
```{r density map}

library(tmap)
tmap_mode("plot")

tm_shape(parcel_proj) +
  tm_dots(
    col = "number_of_bedrooms",
    palette = "YlOrRd",
    style = "cont",
    size = 0.05,
    alpha = 0.5,
    title = "Number of Bedrooms"
  ) +
  tm_layout(
    main.title = "Spatial Distribution of Bedrooms in Philadelphia Homes",
    legend.outside = TRUE
  )



```
Most areas have 2 bedrooms but there are several pockets such as slightly North of Center City and West Philadelphia which have a higher number of bedrooms.


### Phase 3: Feature Engineering

1. **Buffer-based features**:

   - Violent Crimes within 600ft

2. **k-Nearest Neighbor features**:

   - Average distance from parcel to 1, 3, and 5 nearest colleges and university buildings. Code written above in order to map feature.
   
3. **Census variables**:

   - Join median income, percentage with a bachelor's degree, and poverty percentage 

4. **Interaction terms**:

   - Wealthy neighborhoods which take into account if a home is in a wealthy neighborhood which is based on median sale price.
   
5. **Other features**:

   - log_sale_price, log_livable_area, age and exterior condition (if it is good or not)


**Feature Engineering Code**

```{r feature engineering:buffer }
parcel_proj <- parcel_proj %>% filter(!st_is_empty(.)) %>% st_make_valid()
#Number of violent crimes in proximity 
parcel_buffers<- st_buffer(parcel_proj, dist=600)

violent_crimes <- c(
  "Homicide - Criminal",
  "Aggravated Assault No Firearm",
  "Robbery Firearm",
  "Aggravated Assault Firearm")

violent_proj <- crime_proj %>%
  filter(text_gener %in% violent_crimes)
    
violent_crime_counts <- st_intersects(parcel_buffers, violent_proj)
violent_crime_counts <- lengths(violent_crime_counts)

#join by row numner
parcel_data <- parcel_data %>%
  mutate(violent_crime_600ft = violent_crime_counts[1:nrow(parcel_data)])



```

```{r}
#relationship to age
parcel_data <- parcel_data %>%
  mutate(year_built = as.numeric(year_built))%>%
  mutate(Age = 2025 - year_built)%>% filter(Age <2000)

```

```{r}

parcel_data <- parcel_data %>%
  filter(exterior_condition != 0) %>%
  mutate(
    exterior_good = case_when(
      exterior_condition >= 1 & exterior_condition <= 5 ~ 1,
      exterior_condition >= 6 & exterior_condition <= 9 ~ 0,
      TRUE ~ NA_real_ 
    )
  )


```

```{r interaction: as.factor}
#interaction effects
parcel_proj <- st_transform(parcel_data, 3365)
neighborhood_proj <-st_transform(neighborhoods, 3365)

parcel_with_neighborhood <-  st_join(parcel_proj, neighborhood_proj, join = st_within)%>%
  st_drop_geometry()%>%
  group_by(NAME) %>%
  summarize(median_price = median(sale_price, na.rm = TRUE))


wealthy_neighborhoods <- parcel_with_neighborhood%>%
  filter(median_price >= 275500)%>%
  pull(NAME)

#changed wealthy_list to wealthy_neighborhoods because wealthy_list did not exist 
parcel_data <- parcel_proj %>%
  st_join(neighborhood_proj, join = st_within) %>%
  mutate(
    wealthy_neighborhood = ifelse(NAME %in% wealthy_neighborhoods, "Wealthy", "Not Wealthy"),
    wealthy_neighborhood = as.factor(wealthy_neighborhood)
  )
parcel_data <- parcel_data %>%
  relocate(wealthy_neighborhood, .after = MAPNAME)
View(st_drop_geometry(parcel_data))

```


```{r}
#price vs. spatial features

plot(parcel_data$violent_crime_600ft, parcel_data$sale_price)
plot(parcel_data$college_nn1, parcel_data$sale_price)
plot(parcel_data$wealthy_neighborhood, parcel_data$sale_price)

```


Creating Summary Table
```{r}
#Summary table of engineered features

# Create a data frame summarizing each feature
feature_summary <- data.frame(
  Feature_Name = c(
    "violent_crime_600ft",
    "college_nn1",
    "college_nn3",
    "college_nn5",
    "median_incomeE",
    "percentage_bach",
    "percentage_pov",
    "wealthy_neighborhood",
    "log_sale_price",
    "log_livable_area",
    "Age",
    "exterior_good"
  ),
  Feature_Type = c(
    "Buffer-based (spatial)",
    "kNN (spatial distance)",
    "kNN (spatial distance)",
    "kNN (spatial distance)",
    "Census (socioeconomic)",
    "Census (education)",
    "Census (poverty)",
    "Interaction term (categorical)",
    "Transformation (continuous)",
    "Transformation (continuous)",
    "Structural (numeric)",
    "Condition (binary)"
  ),
  Description = c(
    "Count of violent crimes within 600 ft of parcel",
    "Average distance to nearest college/university (k=1)",
    "Average distance to 3 nearest colleges/universities",
    "Average distance to 5 nearest colleges/universities",
    "Median household income of census tract",
    "Percent of tract population with bachelor's degree",
    "Percent of tract population below poverty line",
    "Indicates parcels located in wealthy neighborhoods",
    "Natural log of sale price",
    "Natural log of total livable area (sq ft)",
    "Years since building construction",
    "1 = good/average exterior, 0 = poor"
  ),
  Justification = c(
    "Captures neighborhood safety and crime exposure",
    "Measures proximity to educational amenities",
    "Captures local educational accessibility",
    "Captures broader proximity to higher education hubs",
    "Represents economic conditions of local area",
    "Education level could be connected to property values",
    "Reflects socioeconomic disadvantage",
    "Identifies context of neighborhood wealth levels",
    "Stabilizes variance for regression modeling",
    "Normalizes skewed size variable for modeling",
    "Accounts for devaluation of property value with time",
    "Changed categorical condition to numeric to understand exterior desirability"
  )
)

# Print table nicely
library(knitr)
kable(feature_summary, caption = "Summary of Engineered Features", align = "l")


```

**Justification:**
The features included were engineered to capture social, economic, and environmental factors influencing housing prices. Buffer-based and nearest-neighbor measures quantify local accessibility and safety. Census data helps to enrich parcels with socio-economic context. Interaction terms account for neighborhood-level wealth effects which helps contextualize external features of a home, for instance homes in a wealthy neighborhood are more likely to have a higher property value than those not in a wealthy neighborhood. Applying a binary indicator improves model interpretability and performance for external condition. A more individualized reason for including each feature is included in the table above.

---
### Phase 4: Model Building

**Build models progressively: (for example)**

1. Structural features only
2. + Census variables
3. + Spatial features
4. + Interactions and fixed effects

**For appendix:**

- Complete model code
- Full stargazer/modelsummary output
- Coefficient interpretations

```{r model builder code and summary}
model1 <- lm(log_sale_price ~ number_of_bathrooms + number_of_bedrooms + log_livable_area + garage_spaces + Age + I(Age^2) + exterior_good, data = parcel_data)

summary(model1)

model2 <- lm(log_sale_price ~ number_of_bathrooms + number_of_bedrooms + log_livable_area  + garage_spaces + Age + I(Age^2) + exterior_good + median_incomeE + percentage_bach + percentage_pov, data = parcel_data)

summary(model2)

model3 <- lm(log_sale_price ~ number_of_bathrooms+ garage_spaces + Age + I(Age^2) + exterior_good + log_livable_area + median_incomeE +percentage_bach + percentage_pov + college_nn1 + violent_crime_600ft, data = parcel_data)

summary(model3)

model4 <- lm(log_sale_price ~ number_of_bathrooms+ garage_spaces + Age + I(Age^2) + exterior_good + log_livable_area * wealthy_neighborhood + median_incomeE +percentage_bach + percentage_pov + college_nn1 + violent_crime_600ft, data = parcel_data)

summary(model4)

plot(fitted(model1), residuals(model1))

plot(fitted(model2), residuals(model2))

plot(fitted(model2), residuals(model2))

plot(fitted(model3), residuals(model3))

```

***Coefficient interpretations:***
---

### Phase 5: Model Validation

```{r 10-fold cross-validation with scatter plots}

parcel_data <- na.omit(parcel_data)

ctrl <- trainControl(
  method = "cv",
  number = 10  # 10-fold CV
)

model_cv1 <-  train(log_sale_price ~ number_of_bathrooms + number_of_bedrooms + log_livable_area + garage_spaces + Age + I(Age^2) + exterior_good,
  data = parcel_data,
  method = "lm",
  trControl = ctrl)

model_cv1
  
model_cv2 <- train(log_sale_price ~ number_of_bathrooms + number_of_bedrooms + log_livable_area  + garage_spaces + Age + I(Age^2) + exterior_good + median_incomeE + percentage_bach + percentage_pov,
  data = parcel_data,
  method = "lm",
  trControl = ctrl
)

model_cv2

model_cv3 <- train(log_sale_price ~ number_of_bathrooms + garage_spaces + Age + I(Age^2) + exterior_good + log_livable_area + median_incomeE + percentage_bach + percentage_pov + college_nn1 + violent_crime_600ft,
  data = parcel_data,
  method = "lm",
  trControl = ctrl
)

model_cv3


model_cv4 <- train(log_sale_price ~ number_of_bathrooms+ garage_spaces + Age + I(Age^2) + exterior_good + log_livable_area * wealthy_neighborhood + median_incomeE +percentage_bach + percentage_pov + college_nn1 + violent_crime_600ft,
  data = parcel_data,
  method = "lm",
  trControl = ctrl
)

model_cv4


#Impact of each feature summary
summary(model_cv4$finalModel)

```


```{r predicted vs actual}

library(ggplot2)

ggplot(parcel_data, aes(x = predict(model_cv4$finalModel), y = log_sale_price)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(x = "Predicted log(Sale Price)", y = "Actual log(Sale Price)",
       title = "Predicted vs Actual Sale Prices") +
  theme_minimal()





```


**Discuss which features matter the most:**
The features which matter the most are wealthy neighborhoods, log of livable areas, percentage with a bachelor's degree and percentage of poverty. The First three features have a positive effect while the percentage of poverty has a strong inverse effect.

---
### Phase 6: Model Diagnostics 

**Check assumptions for best model:**

- Residual plot (linearity, homoscedasticity)
- Q-Q plot (normality)
- Cook's distance (influential observations)

```{r residual plot, q-q plot, and cooks distance}

# Residuals vs Fitted
plot(
  fitted(model_cv4),
  residuals(model_cv4),
  main = "Residuals vs Fitted Values",
  xlab = "Fitted Values (Predicted Log Sale Price)",
  ylab = "Residuals",
  pch = 19,
  col = "darkblue"
)
abline(h = 0, col = "red", lwd = 2, lty = 2)

# Q-Q plot for normality
qqnorm(residuals(model_cv4)); qqline(residuals(model_cv4))


# Cook's distance for the final lm model
plot(cooks.distance(model4), type="h", main="Cook's Distance")



```

**Interpretation of plots** 
The Residuals vs Fitted plot is around 0 but slightly curved. This means that it could be missing a non-linear term. We tried accounting for this by squaring the age of the home. 

The Q-Q Plot shows that the model roughly follows a normal distribution which means using a linear regression makes sense. Ultimately this provides more confidence in our model.

Cooks distance shows many spikes that are the same with a few all ones. A few tall spikes is not concerning as most of them are around the same height but the one on the right side of the graph could indicate a possible outlier that was not accounted for.



### Phase 7: Conclusions & Recommendations


```{r}
#neighborhoods that are hardest to predict 
parcel_data$residuals <- residuals(model_cv4$finalModel)
parcel_data %>%
  group_by(LISTNAME) %>%
  summarize(mean_abs_resid = mean(abs(residuals))) %>%
  arrange(desc(mean_abs_resid))


```

**Detailed Discussion:**
The final model explains approximately 59% of the variation in housing prices across Philadelphia, indicating a moderately strong fit. The Mean Absolute Error (MAE) of 0.34 suggests that, on average, the modelâ€™s predictions are off by about 34% of the true log-transformed sale price, which translates to an average deviation of roughly 62% from the actual sale price when converted back to dollar terms. This level of accuracy is reasonable given the complexity and variability of the housing market. The most influential predictors of housing prices include the wealthy neighborhoods, log of livable areas, percentage with a bachelor's degree, all of which have strong positive effects on property values. Conversely, the percentage of residents in poverty show strong negative relationships with home prices, underscoring the social and safety dimensions that influence property markets.

Neighborhoods that are the hardest to predict tend to be those undergoing rapid change or instability, particularly areas experiencing gentrification, high prevalence of vacant and distressed properties or smaller sample areas. Examples of this are Nicetown, Fairhill, and Upper Kensington which are all located in North Philadelphia. In gentrifying areas, rising educational attainment and declining crime rates occur alongside persistent vacancy and poverty, leading to non-linear effects that standard models struggle to account for. The rapid transformation in these neighborhoods contributes to greater residual errors.

From an equity perspective, including socioeconomic indicators such as poverty and education levels improves predictive accuracy but also risks reproducing structural inequalities. By learning from historical patterns of disinvestment, the model may undervalue properties in low-income or historically marginalized neighborhoods. This raises ethical and policy concerns, as automated valuation models could inadvertently reinforce existing disparities. To mitigate this, residual analysis can be used to test for spatial bias, identifying neighborhoods where the model systematically under- or over-predicts values. Recognizing these limitations highlights opportunities to inform equitable housing policy, such as targeting investment or development funds toward historically undervalued areas rather than further harming them through biased valuation tools.


