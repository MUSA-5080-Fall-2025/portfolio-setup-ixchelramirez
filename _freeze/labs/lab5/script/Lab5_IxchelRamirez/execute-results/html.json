{
  "hash": "6ef95956a45bc3d752140932405fcfa7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Space-Time Prediction of Bike Share Demand: Philadelphia Indego\"\nauthor: \"Ixchel Ramirez\"\ndate: \"2025-12-01\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n    code_folding: show\n    code_download: true\n---\n\n\n\n# Introduction\n\n## The Rebalancing Challenge in Philadelphia\n\nThis lab seeks to create a model to solve Philadelphia's Indego bike share operational challenge of **rebalancing bikes to meet anticipated demand**. I am an Indego operations manager at at 6:00 AM on a Monday morning with the following constraints:\n  - 200 stations across Philadelphia\n  - Limited trucks and staff for moving bikes\n  - 2-3 hours before morning rush hour demand peaks\n\nand I need to identify which stations will run out by 8:30am. \n\n---\n\n# Setup\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\nlibrary(grid)\nlibrary(gridExtra)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\n```\n:::\n\n\n## Define Themes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n## Set Census API Key\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_api_key(\"c6a297963a8439019874bbdbc08bd1f0b462f928\", overwrite = TRUE, install = TRUE)\n```\n:::\n\n\n\n\n---\n\n## Part 1: Code Adaption and Comparison \n\nPart 1 compares Q2 to Q1 bike data. I chose Q2 because this is the Spring quarter. Thus, the weather is nicer so it is more likely that people will ride bikes, which would increase the demand.\n\n\n# Data Import & Preparation\n\n## Load Indego Trip Data (Q2 2025)\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read Q1 2025 data\nindego <- read_csv(here(\"data/indego-trips-2025-q2.csv\"))\n\n# Quick look at the data\nglimpse(indego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 365,760\nColumns: 15\n$ trip_id             <dbl> 1164246461, 1164246634, 1164246553, 1164246521, 11…\n$ duration            <dbl> 11, 31, 9, 3, 11, 7, 13, 12, 3, 58, 21, 5, 15, 10,…\n$ start_time          <chr> \"4/1/2025 0:04\", \"4/1/2025 0:04\", \"4/1/2025 0:17\",…\n$ end_time            <chr> \"4/1/2025 0:15\", \"4/1/2025 0:35\", \"4/1/2025 0:26\",…\n$ start_station       <dbl> 3022, 3040, 3396, 3054, 3280, 3301, 3158, 3374, 33…\n$ start_lat           <dbl> 39.95472, 39.96289, 39.92327, 39.96250, 39.93968, …\n$ start_lon           <dbl> -75.18323, -75.16606, -75.18210, -75.17420, -75.21…\n$ end_station         <dbl> 3064, 3100, 3349, 3235, 3349, 3051, 3028, 3154, 33…\n$ end_lat             <dbl> 39.93840, 39.92777, 39.93651, 39.96000, 39.93651, …\n$ end_lon             <dbl> -75.17327, -75.15103, -75.18621, -75.16510, -75.18…\n$ bike_id             <chr> \"31751\", \"14481\", \"02724\", \"24841\", \"25744\", \"3123…\n$ plan_duration       <dbl> 30, 30, 30, 365, 30, 365, 1, 30, 30, 30, 30, 30, 1…\n$ trip_route_category <chr> \"One Way\", \"One Way\", \"One Way\", \"One Way\", \"One W…\n$ passholder_type     <chr> \"Indego30\", \"Indego30\", \"Indego30\", \"Indego365\", \"…\n$ bike_type           <chr> \"electric\", \"standard\", \"standard\", \"electric\", \"e…\n```\n\n\n:::\n:::\n\n\n## Examine the Data Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q2 2025:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q2 2025: 365760 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1743465840 to 1751327820 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip Duration\ncat(\"Duration:\", \n    min(indego$duration), \"to\", \n    max(indego$duration), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDuration: 1 to 1440 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 273 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    341060      24700 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  Day Pass   Indego30  Indego365 IndegoFlex       NULL    Walk-up \n     15712     189135     132693          1          4      28215 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  234502   131258 \n```\n\n\n:::\n:::\n\nInitially there are 365760 observations and 15 variables \n\n## Create Time Bins\n\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0),\n)\n    \n\n# Look at temporal features\nhead(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2025-04-01 00:04:00 2025-04-01 00:00:00    13 Tue       0       0\n2 2025-04-01 00:04:00 2025-04-01 00:00:00    13 Tue       0       0\n3 2025-04-01 00:17:00 2025-04-01 00:00:00    13 Tue       0       0\n4 2025-04-01 00:20:00 2025-04-01 00:00:00    13 Tue       0       0\n5 2025-04-01 00:25:00 2025-04-01 00:00:00    13 Tue       0       0\n6 2025-04-01 00:40:00 2025-04-01 00:00:00    13 Tue       0       0\n```\n\n\n:::\n:::\n\n\n---\n\n# Exploratory Analysis\n\n## Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q2 2025\",\n    subtitle = \"Spring demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/trips_over_time-1.png){width=672}\n:::\n:::\n\n\n**Ridership Observations:** Ridership Increases in May\n\nThe greatest increase in ridership is from May to April. From May onward, the trend line stays pretty consistent through the rest of the quarter showing how during warmer seasons the bikes get more use. There are a few shard decreases in May which causes a slight wave in the trend line.\n\n## Hourly Patterns\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/hourly_patterns-1.png){width=672}\n:::\n:::\n\n\n**Peak Hours:** Comparing Ridership Trends on the Weekend vs the Weekday\nThere are sharp peeks during the weekdays versus on the weekend there is more gradual incline. In the morning there is a peak around 9am and in the evening there is one around 5pm, this means people are likely using the bikes to get to and from work. The peak hours are the same as in Q1, but there are higher level of overall ridership. For Q1 the amount of trips taken at the first peak is around 200 and 300 for the second peak while for Q2 the first peak has 300 trips per hour and around 500 for the second peak. \n\n## Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 6,278 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 4,751 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 4,181 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 4,028 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,163 </td>\n   <td style=\"text-align:right;\"> 39.94974 </td>\n   <td style=\"text-align:right;\"> -75.18097 </td>\n   <td style=\"text-align:right;\"> 4,000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 3,946 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 3,914 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 3,899 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 3,856 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 3,767 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,161 </td>\n   <td style=\"text-align:right;\"> 39.95486 </td>\n   <td style=\"text-align:right;\"> -75.18091 </td>\n   <td style=\"text-align:right;\"> 3,629 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 3,552 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 3,516 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,213 </td>\n   <td style=\"text-align:right;\"> 39.93887 </td>\n   <td style=\"text-align:right;\"> -75.16663 </td>\n   <td style=\"text-align:right;\"> 3,332 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 3,320 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,012 </td>\n   <td style=\"text-align:right;\"> 39.94218 </td>\n   <td style=\"text-align:right;\"> -75.17747 </td>\n   <td style=\"text-align:right;\"> 3,264 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,007 </td>\n   <td style=\"text-align:right;\"> 39.94517 </td>\n   <td style=\"text-align:right;\"> -75.15993 </td>\n   <td style=\"text-align:right;\"> 3,242 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,061 </td>\n   <td style=\"text-align:right;\"> 39.95425 </td>\n   <td style=\"text-align:right;\"> -75.17761 </td>\n   <td style=\"text-align:right;\"> 3,193 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 3,170 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,296 </td>\n   <td style=\"text-align:right;\"> 39.95134 </td>\n   <td style=\"text-align:right;\"> -75.16758 </td>\n   <td style=\"text-align:right;\"> 3,088 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\nThe station that the most trips start from is 3,010, this station has about 30% more trips start at it than the second most popular station, \n\n---\n\n# Get Philadelphia Spatial Context\n\n## Loading Philadelphia Census Data to add demographic context\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- suppressMessages(\n  get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  progress = FALSE,\n  output = \"wide\"\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n)\n# Check the data\nglimpse(philly_census)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408\nColumns: 17\n$ GEOID                  <chr> \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   <chr> \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              <dbl> 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            <dbl> 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                <dbl> 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            <dbl> 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        <dbl> 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            <dbl> 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      <dbl> 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            <dbl> 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              <dbl> 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            <dbl> 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         <dbl> 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            <dbl> 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit <dbl> 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          <dbl> 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n```\n\n\n:::\n:::\n\n\n## Map Philadelphia Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/map_philly-1.png){width=672}\n:::\n:::\n\nBike share demand is clusted around Center City which has a higher median income.The clustering in this map shows where bike stations are located. \n\n\n## Join Census Data to Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/join_census_to_stations-1.png){width=672}\n:::\n:::\n\n**Interpretation of Red X Marks:** The red X marks indicate where stations did not join to census tracts i.e. no one lives in these locations. The X's in the North West are in Fairmount Park, while those in the South are in the Navy Yard, FDR Park and sporting arenas. \n\n# Dealing with missing data\n\nNon-residential bike share stations are removed in the following code. I tried making a feature for non-residential bike share stations data but it did not improve the model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n\n# Hourly Weather Data for Philly\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q1 2025: January 1 - March 31 changed for Q2\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2025-04-01\",\n  date_end = \"2025-06-30\"\n)\n\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature     Precipitation       Wind_Speed    \n Min.   : 33.00   Min.   :0.00000   Min.   : 0.000  \n 1st Qu.: 57.00   1st Qu.:0.00000   1st Qu.: 5.000  \n Median : 65.00   Median :0.00000   Median : 8.000  \n Mean   : 64.69   Mean   :0.01121   Mean   : 8.103  \n 3rd Qu.: 72.00   3rd Qu.:0.00010   3rd Qu.:10.000  \n Max.   :100.00   Max.   :1.14000   Max.   :40.000  \n```\n\n\n:::\n:::\n\n\n## Visualize Weather Patterns\n\nWho is ready for a Philly winter?!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q12 2025\",\n    subtitle = \"Spring to Summer Transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/visualize_weather-1.png){width=672}\n:::\n:::\n\nTemperatures begin to rise from April to May. From May to June there is mild fluctuation. There is a consistent increase in temperatures from June to July.\n\n---\n\n# Create Space-Time Panel\n\n## Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n\n# How many station-hour observations?\nnrow(trips_panel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 179107\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 252\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2177\n```\n\n\n:::\n:::\n\n\nThere are almost 60000 (116718 in winter and 178173 in the Spring), more station-hour observations in Q2 than Q1. \n\n\n## Create Complete Panel Structure\n\nNot every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 548,604 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 179,107 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 369,497 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 548,604 \n```\n\n\n:::\n:::\n\n\n## Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %>% select(Trip_Count, Temperature, Precipitation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Trip_Count       Temperature    Precipitation  \n Min.   : 0.0000   Min.   : 33.0   Min.   :0.000  \n 1st Qu.: 0.0000   1st Qu.: 57.0   1st Qu.:0.000  \n Median : 0.0000   Median : 65.0   Median :0.000  \n Mean   : 0.6055   Mean   : 64.7   Mean   :0.011  \n 3rd Qu.: 1.0000   3rd Qu.: 72.0   3rd Qu.:0.000  \n Max.   :27.0000   Max.   :100.0   Max.   :1.140  \n                   NA's   :6048    NA's   :6048   \n```\n\n\n:::\n:::\n\n\n---\n\n# Create Temporal Lag Variables\n\nThe key innovation for space-time prediction: **past demand predicts future demand**.\n\n## Why Lags?\n\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 727,776 \n```\n\n\n:::\n:::\n\n\n## Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station <- study_panel_complete %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/lag_correlations-1.png){width=672}\n:::\n:::\n\nThe graph for the spring is more varied than for the winter \n\n---\n\n# Temporal Train/Test Split\n\n**CRITICAL:** We must train on PAST data and test on FUTURE data!\n\n## Why Temporal Validation Matters\n\nIn real operations, at 6:00 AM on March 15, we need to predict demand for March 15-31. We have data from Jan 1 - March 14, but NOT from March 15-31 (it hasn't happened yet!).\n\n**Wrong approach:** Train on weeks 10-13, test on weeks 1-9 (predicting past from future!)\n\n**Correct approach:** Train on weeks 1-9, test on weeks 10-13 (predicting future from past)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# for the spring train on 13-22 \n#Test on 23-26\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 23) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 23) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 23)\n\ntest <- study_panel_complete %>%\n  filter(week >= 23)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 498,747 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 220,365 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 20179 to 20242 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 20243 to 20269 \n```\n\n\n:::\n:::\n\n\n---\n\n# Build Predictive Models\n\nWe'll build 5 models with increasing complexity to see what improves predictions.\n\n## Model 1: Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5973 -0.6602 -0.2101  0.2056 25.5474 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.6377352  0.0135543 -47.050 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0411708  0.0110414  -3.729             0.000192 ***\nas.factor(hour)2  -0.0639198  0.0111100  -5.753   0.0000000087548969 ***\nas.factor(hour)3  -0.1045973  0.0111883  -9.349 < 0.0000000000000002 ***\nas.factor(hour)4  -0.0857670  0.0111916  -7.664   0.0000000000000181 ***\nas.factor(hour)5   0.0136237  0.0110296   1.235             0.216759    \nas.factor(hour)6   0.2345878  0.0112929  20.773 < 0.0000000000000002 ***\nas.factor(hour)7   0.4903620  0.0110199  44.498 < 0.0000000000000002 ***\nas.factor(hour)8   0.8226332  0.0109747  74.957 < 0.0000000000000002 ***\nas.factor(hour)9   0.6066977  0.0109072  55.624 < 0.0000000000000002 ***\nas.factor(hour)10  0.4822213  0.0109270  44.131 < 0.0000000000000002 ***\nas.factor(hour)11  0.5251388  0.0107154  49.008 < 0.0000000000000002 ***\nas.factor(hour)12  0.5449794  0.0108905  50.042 < 0.0000000000000002 ***\nas.factor(hour)13  0.5447782  0.0107258  50.791 < 0.0000000000000002 ***\nas.factor(hour)14  0.5785329  0.0106672  54.235 < 0.0000000000000002 ***\nas.factor(hour)15  0.6899547  0.0107275  64.317 < 0.0000000000000002 ***\nas.factor(hour)16  0.8418613  0.0107744  78.136 < 0.0000000000000002 ***\nas.factor(hour)17  1.1067064  0.0106875 103.552 < 0.0000000000000002 ***\nas.factor(hour)18  0.8956767  0.0110737  80.883 < 0.0000000000000002 ***\nas.factor(hour)19  0.6214480  0.0108481  57.286 < 0.0000000000000002 ***\nas.factor(hour)20  0.3792801  0.0110807  34.229 < 0.0000000000000002 ***\nas.factor(hour)21  0.2415679  0.0110556  21.850 < 0.0000000000000002 ***\nas.factor(hour)22  0.1619637  0.0113139  14.315 < 0.0000000000000002 ***\nas.factor(hour)23  0.0797368  0.0108465   7.351   0.0000000000001964 ***\ndotw_simple2       0.0447468  0.0060084   7.447   0.0000000000000954 ***\ndotw_simple3      -0.0390441  0.0059647  -6.546   0.0000000000592243 ***\ndotw_simple4       0.0367432  0.0058631   6.267   0.0000000003686729 ***\ndotw_simple5      -0.0190780  0.0058151  -3.281             0.001035 ** \ndotw_simple6      -0.0395519  0.0060522  -6.535   0.0000000000636339 ***\ndotw_simple7      -0.0593200  0.0062383  -9.509 < 0.0000000000000002 ***\nTemperature        0.0135446  0.0001686  80.354 < 0.0000000000000002 ***\nPrecipitation     -0.0830165  0.0215151  -3.859             0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.104 on 498715 degrees of freedom\nMultiple R-squared:  0.108,\tAdjusted R-squared:  0.1079 \nF-statistic:  1947 on 31 and 498715 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nThe model uses Monday as the baseline. Each coefficient represents the difference \nin expected trips per station-hour compared to Monday - dow_simple2 = Tuesday..\n\n**Weekday Pattern (Tue-Fri):**\n\n- All weekdays except Friday have positive coefficients (0.047 to 0.008), Friday is negative with -0.017.\n- Tuesday has the highest weekday effect (+0.047)\n- Monday through Thursday likely benefit from concentrated commuting patterns, some of the changes in of less people in the office on Fridays could be due to more flexibility around working in the office and Summer Fridays which start in June\n\n**Weekend Pattern (Sat-Sun):**\n\n- Both weekend days have negative coefficients (-0.027 and -0.068)\n- This means FEWER trips per station-hour than Monday\n\n\n**Hourly Interpretation**\n\nHour   Coefficient   Interpretation\n0      (baseline)    0.000 trips/hour (midnight)\n1 to 4am are less than midnight ranging from     -0.043 to -0.110 \n...\nAt 5am people start commuting  0.005, this continues to increase until the morning rush peak +0.809. At 9 am there is a post-rush cool down  +0.592\n...\n15 or 3pm the afternoon picks up +0.666 and peaks at hour 17 aka 5PM with  +1.094   \n18     +0.893       evening declining. The evening peak is higher than the morning peak\n...\n23     +0.076      late night minimal\n\n\n## Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.7651  -0.4068  -0.1187   0.1067  20.1518 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.2468884  0.0115469 -21.381 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0134365  0.0093749  -1.433             0.151789    \nas.factor(hour)2  -0.0040216  0.0094359  -0.426             0.669962    \nas.factor(hour)3  -0.0300068  0.0095050  -3.157             0.001594 ** \nas.factor(hour)4  -0.0126009  0.0095152  -1.324             0.185410    \nas.factor(hour)5   0.0673110  0.0093788   7.177    0.000000000000714 ***\nas.factor(hour)6   0.2357904  0.0096053  24.548 < 0.0000000000000002 ***\nas.factor(hour)7   0.3775938  0.0093781  40.263 < 0.0000000000000002 ***\nas.factor(hour)8   0.5690617  0.0093518  60.851 < 0.0000000000000002 ***\nas.factor(hour)9   0.2389521  0.0093037  25.684 < 0.0000000000000002 ***\nas.factor(hour)10  0.1833591  0.0093034  19.709 < 0.0000000000000002 ***\nas.factor(hour)11  0.2430645  0.0091259  26.635 < 0.0000000000000002 ***\nas.factor(hour)12  0.2813190  0.0092694  30.349 < 0.0000000000000002 ***\nas.factor(hour)13  0.2944059  0.0091260  32.260 < 0.0000000000000002 ***\nas.factor(hour)14  0.3142945  0.0090776  34.623 < 0.0000000000000002 ***\nas.factor(hour)15  0.3951557  0.0091321  43.271 < 0.0000000000000002 ***\nas.factor(hour)16  0.4930111  0.0091816  53.695 < 0.0000000000000002 ***\nas.factor(hour)17  0.6811937  0.0091256  74.647 < 0.0000000000000002 ***\nas.factor(hour)18  0.3717998  0.0094783  39.227 < 0.0000000000000002 ***\nas.factor(hour)19  0.2065849  0.0092634  22.301 < 0.0000000000000002 ***\nas.factor(hour)20  0.0622124  0.0094515   6.582    0.000000000046375 ***\nas.factor(hour)21  0.0511982  0.0094059   5.443    0.000000052351743 ***\nas.factor(hour)22  0.0461062  0.0096124   4.797    0.000001614798692 ***\nas.factor(hour)23  0.0272482  0.0092090   2.959             0.003088 ** \ndotw_simple2       0.0195096  0.0051011   3.825             0.000131 ***\ndotw_simple3      -0.0345530  0.0050665  -6.820    0.000000000009121 ***\ndotw_simple4       0.0214327  0.0049775   4.306    0.000016632428861 ***\ndotw_simple5      -0.0122444  0.0049371  -2.480             0.013136 *  \ndotw_simple6      -0.0215405  0.0051397  -4.191    0.000027780809914 ***\ndotw_simple7      -0.0388226  0.0052980  -7.328    0.000000000000234 ***\nTemperature        0.0038062  0.0001452  26.215 < 0.0000000000000002 ***\nPrecipitation     -0.1105215  0.0182675  -6.050    0.000000001448012 ***\nlag1Hour           0.4196719  0.0013114 320.017 < 0.0000000000000002 ***\nlag3Hours          0.1219574  0.0012950  94.173 < 0.0000000000000002 ***\nlag1day            0.1301294  0.0011988 108.550 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9374 on 498712 degrees of freedom\nMultiple R-squared:  0.3571,\tAdjusted R-squared:  0.3571 \nF-statistic:  8149 on 34 and 498712 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n\nModel 1: \nResidual standard error: 1.101 on 438680 degrees of freedom\nMultiple R-squared:  0.1054,\tAdjusted R-squared:  0.1053 \nF-statistic:  1666 on 31 and 438680 DF,  p-value: < 0.00000000000000022\n\nAdding lags improved the R-squared it went from 0.1053 to  0.3553. \n\n## Model 3: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2209 -0.6616 -0.2563  0.4035 20.4632 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.4406970415  0.0363319876  12.130\nas.factor(hour)1          0.0496543983  0.0387077576   1.283\nas.factor(hour)2          0.0557316649  0.0413301325   1.348\nas.factor(hour)3         -0.0854994429  0.0526259771  -1.625\nas.factor(hour)4         -0.0506592117  0.0502917111  -1.007\nas.factor(hour)5          0.0249121749  0.0351554261   0.709\nas.factor(hour)6          0.3080255762  0.0306174303  10.060\nas.factor(hour)7          0.4102664374  0.0283084868  14.493\nas.factor(hour)8          0.6237282228  0.0273206080  22.830\nas.factor(hour)9          0.1229538278  0.0274924099   4.472\nas.factor(hour)10         0.1089465034  0.0278635955   3.910\nas.factor(hour)11         0.1415417179  0.0273330360   5.178\nas.factor(hour)12         0.2058031351  0.0274404196   7.500\nas.factor(hour)13         0.2423025788  0.0272442395   8.894\nas.factor(hour)14         0.1998194065  0.0269288510   7.420\nas.factor(hour)15         0.3146303978  0.0268079668  11.736\nas.factor(hour)16         0.4608457292  0.0267343133  17.238\nas.factor(hour)17         0.7033251763  0.0264420906  26.599\nas.factor(hour)18         0.2792961266  0.0269051672  10.381\nas.factor(hour)19         0.0955337184  0.0270520220   3.531\nas.factor(hour)20        -0.0243192000  0.0279745479  -0.869\nas.factor(hour)21        -0.0248597751  0.0287195075  -0.866\nas.factor(hour)22        -0.0135141145  0.0298326902  -0.453\nas.factor(hour)23        -0.0034317764  0.0304173530  -0.113\ndotw_simple2              0.0323640406  0.0114869781   2.817\ndotw_simple3             -0.0598277266  0.0118480871  -5.050\ndotw_simple4             -0.0088472033  0.0113540073  -0.779\ndotw_simple5             -0.0690814729  0.0112102533  -6.162\ndotw_simple6              0.0356817125  0.0119532435   2.985\ndotw_simple7              0.0059532202  0.0123577029   0.482\nTemperature               0.0074256468  0.0003481431  21.329\nPrecipitation            -0.3450355405  0.0361701753  -9.539\nlag1Hour                  0.3101259828  0.0021643730 143.287\nlag3Hours                 0.0882016341  0.0022259445  39.624\nlag1day                   0.1149544429  0.0021229555  54.148\nMed_Inc.x                 0.0000002947  0.0000001086   2.714\nPercent_Taking_Transit.y -0.0040106436  0.0004076555  -9.838\nPercent_White.y           0.0025740005  0.0002007487  12.822\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                     0.199563    \nas.factor(hour)2                     0.177515    \nas.factor(hour)3                     0.104237    \nas.factor(hour)4                     0.313789    \nas.factor(hour)5                     0.478555    \nas.factor(hour)6         < 0.0000000000000002 ***\nas.factor(hour)7         < 0.0000000000000002 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9           0.0000077443208911 ***\nas.factor(hour)10          0.0000923370105501 ***\nas.factor(hour)11          0.0000002240580230 ***\nas.factor(hour)12          0.0000000000000642 ***\nas.factor(hour)13        < 0.0000000000000002 ***\nas.factor(hour)14          0.0000000000001175 ***\nas.factor(hour)15        < 0.0000000000000002 ***\nas.factor(hour)16        < 0.0000000000000002 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18        < 0.0000000000000002 ***\nas.factor(hour)19                    0.000413 ***\nas.factor(hour)20                    0.384666    \nas.factor(hour)21                    0.386708    \nas.factor(hour)22                    0.650552    \nas.factor(hour)23                    0.910171    \ndotw_simple2                         0.004841 ** \ndotw_simple3               0.0000004433034819 ***\ndotw_simple4                         0.435855    \ndotw_simple5               0.0000000007184786 ***\ndotw_simple6                         0.002835 ** \ndotw_simple7                         0.629990    \nTemperature              < 0.0000000000000002 ***\nPrecipitation            < 0.0000000000000002 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                            0.006652 ** \nPercent_Taking_Transit.y < 0.0000000000000002 ***\nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.211 on 156977 degrees of freedom\n  (341732 observations deleted due to missingness)\nMultiple R-squared:  0.2487,\tAdjusted R-squared:  0.2486 \nF-statistic:  1405 on 37 and 156977 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\nThe R-squared decreased from Model 2  (0.3553) to Model 3 (0.2488). \n\n## Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.2718325 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.2705223 \n```\n\n\n:::\n:::\n\nModel 2 continues to have the best R-squared. \n\nStation fixed effects captures baseline differences in demand across stations (some are just busier than others!).\n\n## Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.2759937 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.2746771 \n```\n\n\n:::\n:::\n\nAlthough fixed effects help capture other information it does not make the R-squared better.\n---\n\n# Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.82 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.62 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.86 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.86 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.90 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/compare_models-1.png){width=672}\n:::\n:::\n\n\nThe temporal lags provided the most improvement it went from .82 to .62.\n\n**Key Takeaways for Comparing Q2 to Q1:**\nFor both quarters the MAE values were the best for the second model and the second best for the first model. However the MAE values are slightly lower for Q1 (the mean absolute errors are  0.5 and 0.6 for Models 2 and 1 respectively) than for Q2. \n\nTemporal patterns are different as it is beginning to warm up in the Spring so there are more trips this could help account for the MAE value being slightly larger for all the models. \n\nTemporal Lags, time and weather are the most important features in the second quarter.\n\n---\n\n# Space-Time Error Analysis\n\n## Observed vs. Predicted\n\nSince the best model was Model 2, this is the model that is used for error analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/obs_vs_pred-1.png){width=672}\n:::\n:::\n\n\n**Model Performance** \nThe model struggles as the observed trips increases. The difference between slopes is about the same across different time period.\n\n## Spatial Error Patterns\n\nAre prediction errors clustered in certain parts of Philadelphia?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine with better layout\nlibrary(gridExtra)\ngrid.arrange(\n  p1, p2, \n  ncol = 2,\n  top = textGrob(\n    \"Model 2 Performance: Errors vs. Demand Patterns\",\n    gp = gpar(fontsize = 16, fontface = \"bold\")\n  )\n)\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/spatial_errors-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/spatial_errors-2.png){width=672}\n:::\n\n```{.r .cell-code}\np1\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/spatial_errors-3.png){width=672}\n:::\n:::\n\n\n**Spatial Distribution of Errors:** The Center of it All\nAround Center City there is a curve that it does poorly predicting this includes areas like Rittenhouse, Logan Circle, Spring Garden and on the east towards Old City. There are also some errors in Manayunk and East Falls but not to the same extent as in Center City . Around Center City there is also the most amount of demand which is why the errors are so high here \n\n## Temporal Error Patterns\n\nWhen are we most wrong?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/temporal_errors-1.png){width=672}\n:::\n:::\n\nThe model struggles most with the PM rush, specifically the weekday rush. The weekday AM rush is also hard to predict.\n\n\n## Errors and Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/errors_demographics-1.png){width=672}\n:::\n:::\n\n\n**Demographic Groups and Equity Implications**\nGeneral Analysis: Across all three plots, the trend lines are shallow, which indicates weak relationships between station-level prediction error (MAE) and neighborhood demographics. However, the slight positive trends suggest that it is harder to predict in predominantly white areas with the highest demand complexity. Since demographics do not have a strong negative impact on the prediction this will not be changed as a feature.\n\n1. Median Income \nThe slight positive slope suggests that stations in higher-income neighborhoods experience slightly larger prediction errors. This may reflect that higher-income areas, especially those near Center City, tend to have more complex, variable demand patterns, especially during commuting peaks and recreational surges. Since these stations also have a higher total volume of rides, even small percentage errors can translate into higher MAE. Importantly, this does not necessarily mean the model is biased against high-income neighborhoods—it may simply reflect the difficulty of modeling highly volatile demand.\n\n2. Transit Usage \nThe slight negative trend indicates that stations in neighborhoods where more residents use public transit tend to have slightly lower prediction errors. These neighborhoods often have more stable, routine mobility patterns, which are easier for the model to learn. The clustering near the origin (low MAE) suggests that in these neighborhoods, demand is more predictable, while a few high-MAE outliers may be stations affected by special events or atypical usage patterns.\n\n3. Percent White \nThe slight positive slope suggests that stations in predominantly White neighborhoods may have marginally higher prediction errors. This aligns with the pattern for income: neighborhoods with higher White populations in Philadelphia often overlap with high-demand, high-variability areas (e.g., Center City, and Graduate Hospital). The more noticeable spread after 50% White indicates that prediction errors become more variable in these areas.\n\n\n---\n\n#New Features\nThe features I add are Perfect biking weather and demand in Center City Stations. I chose these features as the weather increases bike ridership and the model struggled the most to predict the demand for neighborhoods around Center City.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\nlibrary(dplyr)\n \n# --- Neighborhood boundaries ---\nneighborhoods <- st_read(here(\"data/philadelphia-neighborhoods (2).geojson\")) %>%\n  st_make_valid() %>%          # Fix any geometry issues\n  dplyr::select(MAPNAME, geometry)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `philadelphia-neighborhoods' from data source \n  `C:\\Users\\ixche\\Desktop\\PPA\\portfolio-setup-ixchelramirez\\labs\\lab5\\data\\philadelphia-neighborhoods (2).geojson' \n  using driver `GeoJSON'\nSimple feature collection with 159 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -75.28026 ymin: 39.86701 xmax: -74.95576 ymax: 40.13799\nGeodetic CRS:  WGS 84\n```\n\n\n:::\n\n```{.r .cell-code}\n# --- Center City District boundary ---\nCCDBoundary <- st_read(here(\"data/CCD_BOUNDARY.geojson\")) %>%\n  st_transform(4326)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `9618db84-ff0e-48fe-a286-1cf3053fb0dd202041-1-jt4ge7.dv6t' from data source `C:\\Users\\ixche\\Desktop\\PPA\\portfolio-setup-ixchelramirez\\labs\\lab5\\data\\CCD_BOUNDARY.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -75.1803 ymin: 39.9446 xmax: -75.14993 ymax: 39.9626\nGeodetic CRS:  WGS 84\n```\n\n\n:::\n\n```{.r .cell-code}\nstations_sf <- study_panel_complete %>%\n  distinct(start_station, start_lat.x, start_lon.y) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y)) %>%\n  st_as_sf(coords = c(\"start_lon.y\", \"start_lat.x\"), crs = 4326)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations_with_neighborhood <- st_join(\n  stations_sf,\n  neighborhoods,\n  left = TRUE,\n  join = st_within\n) %>%\n  st_drop_geometry()\n\nsum(is.na(stations_with_neighborhood$MAPNAME))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nUsing the neighborhood data we identify neighborhoods in Center City that will have a high station demand\n\n::: {.cell}\n\n```{.r .cell-code}\ncenter_city_neighborhoods <- c(\n  \"Rittenhouse\", \"Logan Square\", \"Spring Garden\",\n  \"Old City\", \"Washington Square West\", \"Chinatown\",\n  \"Market East\", \"Graduate Hospital\", \"Callowhill\"\n)\n\nstudy_panel_complete <- study_panel_complete %>%\n  left_join(\n    stations_with_neighborhood %>% \n      dplyr::select(start_station, MAPNAME),\n    by = \"start_station\"\n  )\n\n#identifying high-demand stations in Center City\ncenter_city_high_demand_stations <- study_panel_complete %>%\n  filter(MAPNAME %in% center_city_neighborhoods) %>%\n  group_by(start_station) %>%\n  summarize(mean_demand = mean(Trip_Count, na.rm = TRUE)) %>%\n  filter(mean_demand >= median(mean_demand)) %>%\n  pull(start_station)\n\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    center_city_high_demand = ifelse(\n      start_station %in% center_city_high_demand_stations, 1, 0\n    )\n  )\n```\n:::\n\n\nThe second new feature is for perfect weather\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    perfect_weather = ifelse(\n      Temperature >= 60 & Temperature <= 75 & Precipitation < 0.01,\n      1, 0\n    )\n  )\n```\n:::\n\n\nOnce again we will train the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    dotw_simple = factor(\n      dotw,\n      levels = c(\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\")\n    ),\n    hour = factor(hour, levels = 0:23)   # <- IMPORTANT for Poisson\n  )\n\ntrain <- study_panel_complete %>% filter(week < 23)\ntest  <- study_panel_complete %>% filter(week >= 23)\n\ncat(\"Train rows:\", nrow(train), \"Test rows:\", nrow(test), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrain rows: 498747 Test rows: 220365 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2_baseline <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nmodel2_enhanced <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    center_city_high_demand + perfect_weather,\n  data = train\n)\n\ncat(\"\\n=== MODEL 2 ENHANCED ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== MODEL 2 ENHANCED ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R-squared:\", round(summary(model2_enhanced)$r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared: 0.3648 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Adj R-squared:\", round(summary(model2_enhanced)$adj.r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAdj R-squared: 0.3648 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Show improvement\nr2_improvement <- summary(model2_enhanced)$r.squared - summary(model2_baseline)$r.squared\ncat(\"\\nR-squared improvement:\", round(r2_improvement, 4), \n    paste0(\"(+\", round(r2_improvement * 100, 2), \"%)\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nR-squared improvement: 0.0077 (+0.77%) \n```\n\n\n:::\n\n```{.r .cell-code}\n# Baseline linear model\ntest <- test %>%\n  mutate(\n    pred_baseline = predict(model2_baseline, newdata = .),\n    pred_enhanced = predict(model2_enhanced, newdata = .)\n  )\n\ntest <- test %>%\n  mutate(\n    abs_error_baseline = abs(Trip_Count - pred_baseline),\n    abs_error_enhanced = abs(Trip_Count - pred_enhanced)\n  )\n\nmae_baseline <- mean(test$abs_error_baseline, na.rm = TRUE)\nmae_enhanced <- mean(test$abs_error_enhanced, na.rm = TRUE)\n\ncat(\"MAE Baseline LM:\", round(mae_baseline, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMAE Baseline LM: 0.62 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MAE Enhanced LM:\", round(mae_enhanced, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMAE Enhanced LM: 0.622 \n```\n\n\n:::\n:::\n\n\nI chose this model because it slightly improves the R-squared values by 0.0077 (+0.77%). Unfortunately, these features increase the MAE value  +0.002. However, the other features I tried (holidays and non-residential stations) impacted the R-squared and MAE values more negatively. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(speedglm)\nlibrary(MASS)\n\nmodel2_poisson <- speedglm(\n  Trip_Count ~ \n    hour + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    center_city_high_demand + perfect_weather,\n  data = train,\n  family = poisson()\n)\n\n\ntest$hour <- factor(test$hour, levels = levels(train$hour))\ntest$dotw_simple <- factor(test$dotw_simple, levels = levels(train$dotw_simple))\n\ntest$pred_poisson <- predict(model2_poisson, newdata = test, type = \"response\")\ntest$abs_error_poisson <- abs(test$Trip_Count - test$pred_poisson)\n\nmae_poisson <- mean(test$abs_error_poisson, na.rm = FALSE)\n\ncat(\"Poisson Test MAE =\", mae_poisson, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPoisson Test MAE = NA \n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"Poisson AIC:\", AIC(model2_poisson), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPoisson AIC: 903426.4 \n```\n\n\n:::\n\n```{.r .cell-code}\npseudo_r2 <- 1 - (model2_poisson$deviance / model2_poisson$null.deviance)\ncat(\"Pseudo R²:\", round(pseudo_r2, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPseudo R²:  \n```\n\n\n:::\n\n```{.r .cell-code}\npearson_resid <- residuals(model2_poisson, type = \"pearson\")\noverdispersion <- sum(pearson_resid^2) / model2_poisson$df.residual\n\ncat(\"Overdispersion:\", round(overdispersion, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOverdispersion:  \n```\n\n\n:::\n:::\n\n\n## Part 4: Critical Reflection \n\nOperational implications:\nThe enhanced model’s MAE is still not reliable enough for Indego to use for automated rebalancing, especially during rush hours such as around 8:30am in Center City, where demand is most volatile. Although incorporating distance to key Center City neighborhoods slightly improved predictions for outer stations, it did not significantly reduce error in the core. Given these limitations, the system should only be used under stable conditions—such as good weather and outside peak periods. Model 5 attempted to capture rush-hour dynamics but the MAE suffered this indicates that a separate model might be needed for this specific area and time period, a hyper-local model.\n\nEquity and model limitations:\nPrediction errors are somewhat higher in higher-income, majority White neighborhoods, likely because these stations experience the greatest fluctuations in demand. While this pattern does not imply bias against underserved areas, there is a risk that Indego could unintentionally reinforce existing disparities if it prioritizes improving accuracy only in already well-served neighborhoods rather than expanding station access. To prevent this, demand prediction should be paired with safeguards such as minimum service standards, equity-based station planning, and periodic evaluations of model performance across demographic groups. To improve the model one might look at data across a larger duration of time, and a schedule of transit disruptions. To capture events it would be helpful to use national holidays in conjunction with sporting events. One of the features I tried was holidays. However, since several holidays fell on the weekend when ridership is lower this did not help improve the accuracy of the model. As Philly is a big sports city, categorizing games more broadly as events might help improve this component. \n---\n\n\n",
    "supporting": [
      "Lab5_IxchelRamirez_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}