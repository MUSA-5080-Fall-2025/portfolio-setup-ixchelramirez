{
  "hash": "04b8e0a49a3e23929f50996dc435e8d2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Space-Time Prediction of Bike Share Demand: Philadelphia Indego\"\nauthor: \"Ixchel Ramirez\"\ndate: \"2025-12-01\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n    code_folding: show\n    code_download: true\n---\n\n\n\n# Introduction\n\n## The Rebalancing Challenge in Philadelphia\n\nThis lab seeks to create a model to solve Philadelphia's Indego bike share operational challenge of **rebalancing bikes to meet anticipated demand**. I am an Indego operations manager at at 6:00 AM on a Monday morning with the following constraints:\n  - 200 stations across Philadelphia\n  - Limited trucks and staff for moving bikes\n  - 2-3 hours before morning rush hour demand peaks\n\nand I need to identify which stations will run out by 8:30am. \n\n---\n\n# Setup\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\nlibrary(grid)\nlibrary(gridExtra)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\n```\n:::\n\n\n## Define Themes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n## Set Census API Key\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_api_key(\"c6a297963a8439019874bbdbc08bd1f0b462f928\", overwrite = TRUE, install = TRUE)\n```\n:::\n\n\n\n\n---\n\n## Part 1: Code Adaption and Comparison \n\nPart 1 compares Q2 to Q1 bike data. I chose Q2 because this is the Spring quarter. Thus, the weather is nicer so it is more likely that people will ride bikes, which would increase the demand.\n\n\n# Data Import & Preparation\n\n## Load Indego Trip Data (Q2 2025)\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read Q1 2025 data\nindego <- read_csv(here(\"data/indego-trips-2025-q2.csv\"))\n\n# Quick look at the data\nglimpse(indego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 365,760\nColumns: 15\n$ trip_id             <dbl> 1164246461, 1164246634, 1164246553, 1164246521, 11…\n$ duration            <dbl> 11, 31, 9, 3, 11, 7, 13, 12, 3, 58, 21, 5, 15, 10,…\n$ start_time          <chr> \"4/1/2025 0:04\", \"4/1/2025 0:04\", \"4/1/2025 0:17\",…\n$ end_time            <chr> \"4/1/2025 0:15\", \"4/1/2025 0:35\", \"4/1/2025 0:26\",…\n$ start_station       <dbl> 3022, 3040, 3396, 3054, 3280, 3301, 3158, 3374, 33…\n$ start_lat           <dbl> 39.95472, 39.96289, 39.92327, 39.96250, 39.93968, …\n$ start_lon           <dbl> -75.18323, -75.16606, -75.18210, -75.17420, -75.21…\n$ end_station         <dbl> 3064, 3100, 3349, 3235, 3349, 3051, 3028, 3154, 33…\n$ end_lat             <dbl> 39.93840, 39.92777, 39.93651, 39.96000, 39.93651, …\n$ end_lon             <dbl> -75.17327, -75.15103, -75.18621, -75.16510, -75.18…\n$ bike_id             <chr> \"31751\", \"14481\", \"02724\", \"24841\", \"25744\", \"3123…\n$ plan_duration       <dbl> 30, 30, 30, 365, 30, 365, 1, 30, 30, 30, 30, 30, 1…\n$ trip_route_category <chr> \"One Way\", \"One Way\", \"One Way\", \"One Way\", \"One W…\n$ passholder_type     <chr> \"Indego30\", \"Indego30\", \"Indego30\", \"Indego365\", \"…\n$ bike_type           <chr> \"electric\", \"standard\", \"standard\", \"electric\", \"e…\n```\n\n\n:::\n:::\n\n\n## Examine the Data Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q2 2025:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q2 2025: 365760 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1743465840 to 1751327820 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip Duration\ncat(\"Duration:\", \n    min(indego$duration), \"to\", \n    max(indego$duration), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDuration: 1 to 1440 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 273 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    341060      24700 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  Day Pass   Indego30  Indego365 IndegoFlex       NULL    Walk-up \n     15712     189135     132693          1          4      28215 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  234502   131258 \n```\n\n\n:::\n:::\n\nInitially there are 365760 observations and 15 variables \n\n#leave this out for the first round\n# Cleaning data  \n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  filter(\n    !(duration <= 5 & start_station == end_station |  passholder_type == \"NULL\"))\n```\n:::\n\nThis chunk of data takes out bike rentals which are counted as trips but weren't trips for things such as a bike not working and needing to be returned. The code filters for trips that are five minutes and under where the start and end station are the same. There are some instances where the start and end location are different for five minutes which could indicate a short bike ride. Cleaning the data brought the observations down to 354596. I also filtered out where the pass-holder type is null as we will be using this category later. \n\n\nExploring data after cleaned\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q2 2025:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q2 2025: 354592 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1743465840 to 1751327820 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 273 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    341057      13535 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  Day Pass   Indego30  Indego365 IndegoFlex    Walk-up \n     14925     182962     129533          1      27171 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  226741   127851 \n```\n\n\n:::\n:::\n\nAll of the trips were classified as round trip because the start and end location were the same even though the user was most likely not able to go anywhere. \n\n\n## Create Time Bins\n\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0),\n)\n    \n\n# Look at temporal features\nhead(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2025-04-01 00:04:00 2025-04-01 00:00:00    13 Tue       0       0\n2 2025-04-01 00:04:00 2025-04-01 00:00:00    13 Tue       0       0\n3 2025-04-01 00:17:00 2025-04-01 00:00:00    13 Tue       0       0\n4 2025-04-01 00:20:00 2025-04-01 00:00:00    13 Tue       0       0\n5 2025-04-01 00:25:00 2025-04-01 00:00:00    13 Tue       0       0\n6 2025-04-01 00:40:00 2025-04-01 00:00:00    13 Tue       0       0\n```\n\n\n:::\n:::\n\n\n---\n\n# Exploratory Analysis\n\n## Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q2 2025\",\n    subtitle = \"Spring demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/trips_over_time-1.png){width=672}\n:::\n:::\n\n\n**Ridership Observations:** Ridership Increases in May\n\nThe greatest increase in ridership is from May to April. From May onward, the trend line stays pretty consistent through the rest of the quarter showing how during warmer seasons the bikes get more use. There are a few shard decreases in May which causes a slight wave in the trend line.\n\n## Hourly Patterns\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/hourly_patterns-1.png){width=672}\n:::\n:::\n\n\n**Peak Hours:** Comparing Ridership Trends on the Weekend vs the Weekday\nThere are sharp peeks during the weekdays versus on the weekend there is more gradual incline. In the morning there is a peak around 9am and in the evening there is one around 5pm, this means people are likely using the bikes to get to and from work. The peak hours are the same as in Q1, but there are higher level of overall ridership. For Q1 the amount of trips taken at the first peak is around 200 and 300 for the second peak while for Q2 the first peak has 300 trips per hour and around 500 for the second peak. \n\n## Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 6,069 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 4,630 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 4,129 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,163 </td>\n   <td style=\"text-align:right;\"> 39.94974 </td>\n   <td style=\"text-align:right;\"> -75.18097 </td>\n   <td style=\"text-align:right;\"> 3,899 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 3,890 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 3,840 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 3,818 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 3,811 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 3,657 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 3,654 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,161 </td>\n   <td style=\"text-align:right;\"> 39.95486 </td>\n   <td style=\"text-align:right;\"> -75.18091 </td>\n   <td style=\"text-align:right;\"> 3,530 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 3,505 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 3,413 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,213 </td>\n   <td style=\"text-align:right;\"> 39.93887 </td>\n   <td style=\"text-align:right;\"> -75.16663 </td>\n   <td style=\"text-align:right;\"> 3,225 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 3,204 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,012 </td>\n   <td style=\"text-align:right;\"> 39.94218 </td>\n   <td style=\"text-align:right;\"> -75.17747 </td>\n   <td style=\"text-align:right;\"> 3,186 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 3,116 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,061 </td>\n   <td style=\"text-align:right;\"> 39.95425 </td>\n   <td style=\"text-align:right;\"> -75.17761 </td>\n   <td style=\"text-align:right;\"> 3,110 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,007 </td>\n   <td style=\"text-align:right;\"> 39.94517 </td>\n   <td style=\"text-align:right;\"> -75.15993 </td>\n   <td style=\"text-align:right;\"> 3,069 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,296 </td>\n   <td style=\"text-align:right;\"> 39.95134 </td>\n   <td style=\"text-align:right;\"> -75.16758 </td>\n   <td style=\"text-align:right;\"> 3,037 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\nThe station that the most trips start from is 3,010, this station has about 30% more trips start at it than the second most popular station, \n\n---\n\n# Get Philadelphia Spatial Context\n\n## Loading Philadelphia Census Data to add demographic context\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the data\nglimpse(philly_census)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408\nColumns: 17\n$ GEOID                  <chr> \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   <chr> \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              <dbl> 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            <dbl> 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                <dbl> 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            <dbl> 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        <dbl> 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            <dbl> 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      <dbl> 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            <dbl> 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              <dbl> 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            <dbl> 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         <dbl> 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            <dbl> 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit <dbl> 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          <dbl> 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n```\n\n\n:::\n:::\n\n\n## Map Philadelphia Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/map_philly-1.png){width=672}\n:::\n:::\n\nBike share demand is clusted around Center City which has a higher median income.The clustering in this map shows where bike stations are located. \n\n\n## Join Census Data to Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/join_census_to_stations-1.png){width=672}\n:::\n:::\n\n**Interpretation of Red X Marks:** The red X marks indicate where stations did not join to census tracts i.e. no one lives in these locations. The X's in the North West are in Fairmount Park, while those in the South are in the Navy Yard, FDR Park and sporting arenas. \n\n# Dealing with missing data\n\nNon-residential bike share stations are removed in the following code. I tried making a feature for non-residential bike share stations data but it did not improve the model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n\n# Hourly Weather Data for Philly\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q1 2025: January 1 - March 31 changed for Q2\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2025-04-01\",\n  date_end = \"2025-06-30\"\n)\n\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature     Precipitation       Wind_Speed    \n Min.   : 33.00   Min.   :0.00000   Min.   : 0.000  \n 1st Qu.: 57.00   1st Qu.:0.00000   1st Qu.: 5.000  \n Median : 65.00   Median :0.00000   Median : 8.000  \n Mean   : 64.69   Mean   :0.01121   Mean   : 8.103  \n 3rd Qu.: 72.00   3rd Qu.:0.00010   3rd Qu.:10.000  \n Max.   :100.00   Max.   :1.14000   Max.   :40.000  \n```\n\n\n:::\n:::\n\n\n## Visualize Weather Patterns\n\nWho is ready for a Philly winter?!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q12 2025\",\n    subtitle = \"Spring to Summer Transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/visualize_weather-1.png){width=672}\n:::\n:::\n\nTemperatures begin to rise from April to May. From May to June there is mild fluctuation. There is a consistent increase in temperatures from June to July.\n\n---\n\n# Create Space-Time Panel\n\n## Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n\n# How many station-hour observations?\nnrow(trips_panel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 178173\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 252\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2177\n```\n\n\n:::\n:::\n\n\nThere are almost 60000 (116718 in winter and 178173 in the Spring), more station-hour observations in Q2 than Q1. \n\n\n## Create Complete Panel Structure\n\nNot every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 548,604 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 178,173 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 370,431 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 548,604 \n```\n\n\n:::\n:::\n\n\n## Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %>% select(Trip_Count, Temperature, Precipitation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Trip_Count       Temperature    Precipitation  \n Min.   : 0.0000   Min.   : 33.0   Min.   :0.000  \n 1st Qu.: 0.0000   1st Qu.: 57.0   1st Qu.:0.000  \n Median : 0.0000   Median : 65.0   Median :0.000  \n Mean   : 0.5869   Mean   : 64.7   Mean   :0.011  \n 3rd Qu.: 1.0000   3rd Qu.: 72.0   3rd Qu.:0.000  \n Max.   :24.0000   Max.   :100.0   Max.   :1.140  \n                   NA's   :6048    NA's   :6048   \n```\n\n\n:::\n:::\n\n\n---\n\n# Create Temporal Lag Variables\n\nThe key innovation for space-time prediction: **past demand predicts future demand**.\n\n## Why Lags?\n\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 727,776 \n```\n\n\n:::\n:::\n\n\n## Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station <- study_panel_complete %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/lag_correlations-1.png){width=672}\n:::\n:::\n\nThe graph for the spring is more varied than for the winter \n\n---\n\n# Temporal Train/Test Split\n\n**CRITICAL:** We must train on PAST data and test on FUTURE data!\n\n## Why Temporal Validation Matters\n\nIn real operations, at 6:00 AM on March 15, we need to predict demand for March 15-31. We have data from Jan 1 - March 14, but NOT from March 15-31 (it hasn't happened yet!).\n\n**Wrong approach:** Train on weeks 10-13, test on weeks 1-9 (predicting past from future!)\n\n**Correct approach:** Train on weeks 1-9, test on weeks 10-13 (predicting future from past)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# for the spring train on 13-22 \n#Test on 23-26\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 23) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 23) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 23)\n\ntest <- study_panel_complete %>%\n  filter(week >= 23)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 498,747 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 220,365 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 20179 to 20242 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 20243 to 20269 \n```\n\n\n:::\n:::\n\n\n---\n\n# Build Predictive Models\n\nWe'll build 5 models with increasing complexity to see what improves predictions.\n\n## Model 1: Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5510 -0.6418 -0.2047  0.2132 22.5913 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.6175752  0.0130647 -47.270 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0396794  0.0106426  -3.728             0.000193 ***\nas.factor(hour)2  -0.0611030  0.0107087  -5.706   0.0000000115796103 ***\nas.factor(hour)3  -0.0997839  0.0107842  -9.253 < 0.0000000000000002 ***\nas.factor(hour)4  -0.0810961  0.0107874  -7.518   0.0000000000000558 ***\nas.factor(hour)5   0.0161955  0.0106312   1.523             0.127660    \nas.factor(hour)6   0.2327152  0.0108850  21.379 < 0.0000000000000002 ***\nas.factor(hour)7   0.4786795  0.0106218  45.066 < 0.0000000000000002 ***\nas.factor(hour)8   0.8031501  0.0105783  75.924 < 0.0000000000000002 ***\nas.factor(hour)9   0.5910807  0.0105132  56.223 < 0.0000000000000002 ***\nas.factor(hour)10  0.4699765  0.0105323  44.622 < 0.0000000000000002 ***\nas.factor(hour)11  0.5118564  0.0103284  49.558 < 0.0000000000000002 ***\nas.factor(hour)12  0.5291640  0.0104972  50.410 < 0.0000000000000002 ***\nas.factor(hour)13  0.5286348  0.0103384  51.133 < 0.0000000000000002 ***\nas.factor(hour)14  0.5618280  0.0102819  54.643 < 0.0000000000000002 ***\nas.factor(hour)15  0.6675923  0.0103400  64.564 < 0.0000000000000002 ***\nas.factor(hour)16  0.8170541  0.0103852  78.675 < 0.0000000000000002 ***\nas.factor(hour)17  1.0780978  0.0103014 104.655 < 0.0000000000000002 ***\nas.factor(hour)18  0.8690251  0.0106737  81.417 < 0.0000000000000002 ***\nas.factor(hour)19  0.6035716  0.0104563  57.723 < 0.0000000000000002 ***\nas.factor(hour)20  0.3693512  0.0106805  34.582 < 0.0000000000000002 ***\nas.factor(hour)21  0.2359489  0.0106563  22.142 < 0.0000000000000002 ***\nas.factor(hour)22  0.1575797  0.0109052  14.450 < 0.0000000000000002 ***\nas.factor(hour)23  0.0801071  0.0104547   7.662   0.0000000000000183 ***\ndotw_simple2       0.0441425  0.0057914   7.622   0.0000000000000250 ***\ndotw_simple3      -0.0362809  0.0057493  -6.311   0.0000000002783391 ***\ndotw_simple4       0.0384460  0.0056513   6.803   0.0000000000102560 ***\ndotw_simple5      -0.0164841  0.0056051  -2.941             0.003272 ** \ndotw_simple6      -0.0397536  0.0058336  -6.815   0.0000000000094668 ***\ndotw_simple7      -0.0588664  0.0060129  -9.790 < 0.0000000000000002 ***\nTemperature        0.0130789  0.0001625  80.499 < 0.0000000000000002 ***\nPrecipitation     -0.0830531  0.0207380  -4.005   0.0000620600061021 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.064 on 498715 degrees of freedom\nMultiple R-squared:  0.1091,\tAdjusted R-squared:  0.1091 \nF-statistic:  1970 on 31 and 498715 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nThe model uses Monday as the baseline. Each coefficient represents the difference \nin expected trips per station-hour compared to Monday - dow_simple2 = Tuesday..\n\n**Weekday Pattern (Tue-Fri):**\n\n- All weekdays except Friday have positive coefficients (0.047 to 0.008), Friday is negative with -0.017.\n- Tuesday has the highest weekday effect (+0.047)\n- Monday through Thursday likely benefit from concentrated commuting patterns, some of the changes in of less people in the office on Fridays could be due to more flexibility around working in the office and Summer Fridays which start in June\n\n**Weekend Pattern (Sat-Sun):**\n\n- Both weekend days have negative coefficients (-0.027 and -0.068)\n- This means FEWER trips per station-hour than Monday\n\n\n**Hourly Interpretation**\n\nHour   Coefficient   Interpretation\n0      (baseline)    0.000 trips/hour (midnight)\n1 to 4am are less than midnight ranging from     -0.043 to -0.110 \n...\nAt 5am people start commuting  0.005, this continues to increase until the morning rush peak +0.809. At 9 am there is a post-rush cool down  +0.592\n...\n15 or 3pm the afternoon picks up +0.666 and peaks at hour 17 aka 5PM with  +1.094   \n18     +0.893       evening declining. The evening peak is higher than the morning peak\n...\n23     +0.076      late night minimal\n\nIsn't this fun!\n\n## Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.1735  -0.3938  -0.1145   0.1036  18.2460 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.2381244  0.0111077 -21.438 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0119021  0.0090181  -1.320             0.186901    \nas.factor(hour)2  -0.0022011  0.0090768  -0.242             0.808395    \nas.factor(hour)3  -0.0267200  0.0091434  -2.922             0.003474 ** \nas.factor(hour)4  -0.0101183  0.0091535  -1.105             0.268983    \nas.factor(hour)5   0.0680773  0.0090222   7.546   0.0000000000000451 ***\nas.factor(hour)6   0.2325999  0.0092402  25.173 < 0.0000000000000002 ***\nas.factor(hour)7   0.3673275  0.0090217  40.716 < 0.0000000000000002 ***\nas.factor(hour)8   0.5542859  0.0089966  61.611 < 0.0000000000000002 ***\nas.factor(hour)9   0.2313353  0.0089503  25.847 < 0.0000000000000002 ***\nas.factor(hour)10  0.1780022  0.0089498  19.889 < 0.0000000000000002 ***\nas.factor(hour)11  0.2366233  0.0087789  26.954 < 0.0000000000000002 ***\nas.factor(hour)12  0.2720146  0.0089169  30.505 < 0.0000000000000002 ***\nas.factor(hour)13  0.2849705  0.0087788  32.461 < 0.0000000000000002 ***\nas.factor(hour)14  0.3046710  0.0087323  34.890 < 0.0000000000000002 ***\nas.factor(hour)15  0.3822997  0.0087845  43.520 < 0.0000000000000002 ***\nas.factor(hour)16  0.4784034  0.0088323  54.165 < 0.0000000000000002 ***\nas.factor(hour)17  0.6631171  0.0087790  75.535 < 0.0000000000000002 ***\nas.factor(hour)18  0.3581479  0.0091185  39.277 < 0.0000000000000002 ***\nas.factor(hour)19  0.1999504  0.0089114  22.438 < 0.0000000000000002 ***\nas.factor(hour)20  0.0609874  0.0090921   6.708   0.0000000000197879 ***\nas.factor(hour)21  0.0506165  0.0090482   5.594   0.0000000221878558 ***\nas.factor(hour)22  0.0449629  0.0092466   4.863   0.0000011586515380 ***\nas.factor(hour)23  0.0291940  0.0088585   3.296             0.000982 ***\ndotw_simple2       0.0189415  0.0049070   3.860             0.000113 ***\ndotw_simple3      -0.0331309  0.0048737  -6.798   0.0000000000106288 ***\ndotw_simple4       0.0214178  0.0047881   4.473   0.0000077094061006 ***\ndotw_simple5      -0.0114875  0.0047492  -2.419             0.015571 *  \ndotw_simple6      -0.0220212  0.0049442  -4.454   0.0000084328236637 ***\ndotw_simple7      -0.0381868  0.0050964  -7.493   0.0000000000000675 ***\nTemperature        0.0036312  0.0001397  25.998 < 0.0000000000000002 ***\nPrecipitation     -0.1089474  0.0175723  -6.200   0.0000000005652542 ***\nlag1Hour           0.4211432  0.0013105 321.350 < 0.0000000000000002 ***\nlag3Hours          0.1221803  0.0012937  94.442 < 0.0000000000000002 ***\nlag1day            0.1311102  0.0011969 109.546 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9017 on 498712 degrees of freedom\nMultiple R-squared:  0.3605,\tAdjusted R-squared:  0.3605 \nF-statistic:  8270 on 34 and 498712 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n\nModel 1: \nResidual standard error: 1.101 on 438680 degrees of freedom\nMultiple R-squared:  0.1054,\tAdjusted R-squared:  0.1053 \nF-statistic:  1666 on 31 and 438680 DF,  p-value: < 0.00000000000000022\n\n**Question:** Did adding lags improve R²? Why or why not?\nAdding lags improved the R-squared it went from 0.1053 to  0.3553. \n\n## Model 3: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7333 -0.6338 -0.2425  0.3857 18.3516 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.4179231593  0.0348030909  12.008\nas.factor(hour)1          0.0416702698  0.0370970394   1.123\nas.factor(hour)2          0.0566843107  0.0396544330   1.429\nas.factor(hour)3         -0.0839867277  0.0503068563  -1.669\nas.factor(hour)4         -0.0409993291  0.0481732310  -0.851\nas.factor(hour)5          0.0399048881  0.0336867137   1.185\nas.factor(hour)6          0.3131716066  0.0293282388  10.678\nas.factor(hour)7          0.4033355777  0.0271283437  14.868\nas.factor(hour)8          0.6154766865  0.0261846511  23.505\nas.factor(hour)9          0.1264783031  0.0263523072   4.800\nas.factor(hour)10         0.1137682609  0.0267069699   4.260\nas.factor(hour)11         0.1485166402  0.0262079900   5.667\nas.factor(hour)12         0.2033138396  0.0263006307   7.730\nas.factor(hour)13         0.2419693873  0.0261213666   9.263\nas.factor(hour)14         0.2025900007  0.0258178639   7.847\nas.factor(hour)15         0.3129022993  0.0257047155  12.173\nas.factor(hour)16         0.4522737270  0.0256247751  17.650\nas.factor(hour)17         0.6948240231  0.0253453530  27.414\nas.factor(hour)18         0.2761556332  0.0257890254  10.708\nas.factor(hour)19         0.1024236442  0.0259336365   3.949\nas.factor(hour)20        -0.0169079781  0.0268058286  -0.631\nas.factor(hour)21        -0.0143111241  0.0275365371  -0.520\nas.factor(hour)22        -0.0095491467  0.0285905974  -0.334\nas.factor(hour)23         0.0067660453  0.0291417981   0.232\ndotw_simple2              0.0334846627  0.0109849639   3.048\ndotw_simple3             -0.0548417227  0.0113331645  -4.839\ndotw_simple4             -0.0042379172  0.0108552486  -0.390\ndotw_simple5             -0.0645811690  0.0107172030  -6.026\ndotw_simple6              0.0330541870  0.0114344725   2.891\ndotw_simple7              0.0018880971  0.0118179093   0.160\nTemperature               0.0070891752  0.0003330038  21.289\nPrecipitation            -0.3424492658  0.0345283394  -9.918\nlag1Hour                  0.3075938121  0.0021507316 143.018\nlag3Hours                 0.0873836505  0.0022089421  39.559\nlag1day                   0.1164552893  0.0021046708  55.332\nMed_Inc.x                 0.0000002807  0.0000001039   2.702\nPercent_Taking_Transit.y -0.0035011271  0.0003898597  -8.980\nPercent_White.y           0.0026873009  0.0001921760  13.984\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                      0.26132    \nas.factor(hour)2                      0.15287    \nas.factor(hour)3                      0.09502 .  \nas.factor(hour)4                      0.39473    \nas.factor(hour)5                      0.23618    \nas.factor(hour)6         < 0.0000000000000002 ***\nas.factor(hour)7         < 0.0000000000000002 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9          0.00000159196787651 ***\nas.factor(hour)10         0.00002046639819536 ***\nas.factor(hour)11         0.00000001457052220 ***\nas.factor(hour)12         0.00000000000001079 ***\nas.factor(hour)13        < 0.0000000000000002 ***\nas.factor(hour)14         0.00000000000000429 ***\nas.factor(hour)15        < 0.0000000000000002 ***\nas.factor(hour)16        < 0.0000000000000002 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18        < 0.0000000000000002 ***\nas.factor(hour)19         0.00007836477910107 ***\nas.factor(hour)20                     0.52820    \nas.factor(hour)21                     0.60326    \nas.factor(hour)22                     0.73838    \nas.factor(hour)23                     0.81640    \ndotw_simple2                          0.00230 ** \ndotw_simple3              0.00000130587022763 ***\ndotw_simple4                          0.69624    \ndotw_simple5              0.00000000168509456 ***\ndotw_simple6                          0.00384 ** \ndotw_simple7                          0.87307    \nTemperature              < 0.0000000000000002 ***\nPrecipitation            < 0.0000000000000002 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                             0.00689 ** \nPercent_Taking_Transit.y < 0.0000000000000002 ***\nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.154 on 156111 degrees of freedom\n  (342598 observations deleted due to missingness)\nMultiple R-squared:  0.2522,\tAdjusted R-squared:  0.252 \nF-statistic:  1423 on 37 and 156111 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\nThe R-squared decreased from Model 2  (0.3553) to Model 3 (0.2488). \n\n## Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.275643 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.2743324 \n```\n\n\n:::\n:::\n\nModel 2 continues to have the best R-squared. \n\n**What do station fixed effects capture?** Baseline differences in demand across stations (some are just busier than others!).\n\n## Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.2799324 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.2786157 \n```\n\n\n:::\n:::\n\nAlthough fixed effects help capture other information it does not make the R-squared better.\n---\n\n# Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.79 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.60 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.82 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.82 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.86 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/compare_models-1.png){width=672}\n:::\n:::\n\n\n**Question:** Which features gave us the biggest improvement?\nThe temporal lags provided the most improvement it went from .82 to .62.\n\n**Key Takeaways for Comparing Q2 to Q1:** Which\nFor both quarters the MAE values were the best for the second model and the second best for the first model. However the MAE values are slightly lower for Q1 (the mean absolute errors are  0.5 and 0.6 for Models 2 and 1 respectively) than for Q2. \n\nTemporal patterns are different as it is beginning to warm up in the Spring so there are more trips this could help account for the MAE value being slightly larger for all the models. \n\nTemporal Lags, time and weather are the most important features in the second quarter.\n\n---\n\n# Space-Time Error Analysis\n\n## Observed vs. Predicted\n\nSince the best model was Model 2, this is the model that is used for error analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/obs_vs_pred-1.png){width=672}\n:::\n:::\n\n\n**Model Performance** \nThe model struggles as the observed trips increases. The difference between slopes is about the same across different time period.\n\n## Spatial Error Patterns\n\nAre prediction errors clustered in certain parts of Philadelphia?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine with better layout\nlibrary(gridExtra)\ngrid.arrange(\n  p1, p2, \n  ncol = 2,\n  top = textGrob(\n    \"Model 2 Performance: Errors vs. Demand Patterns\",\n    gp = gpar(fontsize = 16, fontface = \"bold\")\n  )\n)\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/spatial_errors-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/spatial_errors-2.png){width=672}\n:::\n\n```{.r .cell-code}\np1\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/spatial_errors-3.png){width=672}\n:::\n:::\n\n\n**Spatial Distribution of Errors:** The Center of it All\nAround Center City there is a curve that it does poorly predicting this includes areas like Rittenhouse, Logan Circle, Spring Garden and on the east towards Old City. There are also some errors in Manayunk and East Falls but not to the same extent as in Center City . Around Center City there is also the most amount of demand which is why the errors are so high here \n\n## Temporal Error Patterns\n\nWhen are we most wrong?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/temporal_errors-1.png){width=672}\n:::\n:::\n\nThe model struggles most with the PM rush, specifically the weekday rush. The weekday AM rush is also hard to predict.\n\n\n## Errors and Demographics\n\nAre prediction errors related to neighborhood characteristics?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Lab5_IxchelRamirez_files/figure-html/errors_demographics-1.png){width=672}\n:::\n:::\n\n\n**Critical Question:** Are prediction errors systematically higher in certain demographic groups? What are the equity implications?\nAcross all three plots, the trend lines are shallow, which indicates weak relationships between station-level prediction error (MAE) and neighborhood demographics. However, the slight positive trends suggest that it is harder to predict in predominantly white areas with the highest demand complexity. Since demographics do not have a strong negative impact on the prediction this will not be changed as a feature.\n\n1. Median Income (slight positive slope)\nThe weak upward trend suggests that stations in higher-income neighborhoods experience slightly larger prediction errors. This may reflect that higher-income areas—particularly Center City and adjacent neighborhoods—tend to have more complex, variable demand patterns, especially during commuting peaks and recreational surges. Because these stations also have a higher total volume of rides, even small percentage errors can translate into higher MAE. Importantly, this does not necessarily mean the model is biased against high-income neighborhoods—it may simply reflect the difficulty of modeling highly volatile demand.\n\n2. Transit Usage (slight negative slope)\nThe negative trend indicates that stations in neighborhoods where more residents use public transit tend to have slightly lower prediction errors. These neighborhoods often have more stable, routine mobility patterns, which are easier for the model to learn. The clustering near the origin (low MAE) suggests that in these neighborhoods, demand is more predictable, while a few high-MAE outliers may be stations affected by special events or atypical usage patterns.\n\n3. Percent White (slight positive slope)\nThe slight positive slope suggests that stations in predominantly White neighborhoods may have marginally higher prediction errors. This aligns with the pattern seen for income: neighborhoods with higher White populations in Philadelphia often overlap with high-demand, high-variability areas (e.g., Center City, Graduate Hospital, parts of South Philly). More variability → harder to predict → slightly higher MAE. The more noticeable spread after 50% White indicates that prediction errors become more variable in these areas.\n\n\n---\n\n#New Features\nThe features I add are Perfect biking weather and demand in Center City Stations. I chose these features as the weather increases bike ridership and the model struggled the most to predict the demand for neighborhoods around Center City.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\nlibrary(dplyr)\n \n# --- Neighborhood boundaries ---\nneighborhoods <- st_read(here(\"data/philadelphia-neighborhoods (2).geojson\")) %>%\n  st_make_valid() %>%          # Fix any geometry issues\n  dplyr::select(MAPNAME, geometry)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `philadelphia-neighborhoods' from data source \n  `C:\\Users\\ixche\\Desktop\\PPA\\portfolio-setup-ixchelramirez\\labs\\lab5\\data\\philadelphia-neighborhoods (2).geojson' \n  using driver `GeoJSON'\nSimple feature collection with 159 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -75.28026 ymin: 39.86701 xmax: -74.95576 ymax: 40.13799\nGeodetic CRS:  WGS 84\n```\n\n\n:::\n\n```{.r .cell-code}\n# --- Center City District boundary ---\nCCDBoundary <- st_read(here(\"data/CCD_BOUNDARY.geojson\")) %>%\n  st_transform(4326)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `9618db84-ff0e-48fe-a286-1cf3053fb0dd202041-1-jt4ge7.dv6t' from data source `C:\\Users\\ixche\\Desktop\\PPA\\portfolio-setup-ixchelramirez\\labs\\lab5\\data\\CCD_BOUNDARY.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -75.1803 ymin: 39.9446 xmax: -75.14993 ymax: 39.9626\nGeodetic CRS:  WGS 84\n```\n\n\n:::\n\n```{.r .cell-code}\nstations_sf <- study_panel_complete %>%\n  distinct(start_station, start_lat.x, start_lon.y) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y)) %>%\n  st_as_sf(coords = c(\"start_lon.y\", \"start_lat.x\"), crs = 4326)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations_with_neighborhood <- st_join(\n  stations_sf,\n  neighborhoods,\n  left = TRUE,\n  join = st_within\n) %>%\n  st_drop_geometry()\n\nsum(is.na(stations_with_neighborhood$MAPNAME))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncenter_city_neighborhoods <- c(\n  \"Rittenhouse\", \"Logan Square\", \"Spring Garden\",\n  \"Old City\", \"Washington Square West\", \"Chinatown\",\n  \"Market East\", \"Graduate Hospital\", \"Callowhill\"\n)\n\nstudy_panel_complete <- study_panel_complete %>%\n  left_join(\n    stations_with_neighborhood %>% \n      dplyr::select(start_station, MAPNAME),\n    by = \"start_station\"\n  )\n\n#Might not need if run from the top \n#study_panel_complete <- study_panel_complete %>%\n#  mutate(\n#    MAPNAME = coalesce(MAPNAME.x, MAPNAME.y)\n#  ) %>%\n#  select(-MAPNAME.x, -MAPNAME.y)\n\n#identifying high-demand stations in Center City\ncenter_city_high_demand_stations <- study_panel_complete %>%\n  filter(MAPNAME %in% center_city_neighborhoods) %>%\n  group_by(start_station) %>%\n  summarize(mean_demand = mean(Trip_Count, na.rm = TRUE)) %>%\n  filter(mean_demand >= median(mean_demand)) %>%\n  pull(start_station)\n\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    center_city_high_demand = ifelse(\n      start_station %in% center_city_high_demand_stations, 1, 0\n    )\n  )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    perfect_weather = ifelse(\n      Temperature >= 60 & Temperature <= 75 & Precipitation < 0.01,\n      1, 0\n    )\n  )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    dotw_simple = factor(\n      dotw,\n      levels = c(\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\")\n    ),\n    hour = factor(hour, levels = 0:23)   # <- IMPORTANT for Poisson\n  )\n\ntrain <- study_panel_complete %>% filter(week < 23)\ntest  <- study_panel_complete %>% filter(week >= 23)\n\ncat(\"Train rows:\", nrow(train), \"Test rows:\", nrow(test), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrain rows: 498747 Test rows: 220365 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2_baseline <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nmodel2_enhanced <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    center_city_high_demand + perfect_weather,\n  data = train\n)\n\ncat(\"\\n=== MODEL 2 ENHANCED ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== MODEL 2 ENHANCED ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R-squared:\", round(summary(model2_enhanced)$r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared: 0.3683 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Adj R-squared:\", round(summary(model2_enhanced)$adj.r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAdj R-squared: 0.3683 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Show improvement\nr2_improvement <- summary(model2_enhanced)$r.squared - summary(model2_baseline)$r.squared\ncat(\"\\nR-squared improvement:\", round(r2_improvement, 4), \n    paste0(\"(+\", round(r2_improvement * 100, 2), \"%)\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nR-squared improvement: 0.0078 (+0.78%) \n```\n\n\n:::\n\n```{.r .cell-code}\n# Baseline linear model\ntest <- test %>%\n  mutate(\n    pred_baseline = predict(model2_baseline, newdata = .),\n    pred_enhanced = predict(model2_enhanced, newdata = .)\n  )\n\ntest <- test %>%\n  mutate(\n    abs_error_baseline = abs(Trip_Count - pred_baseline),\n    abs_error_enhanced = abs(Trip_Count - pred_enhanced)\n  )\n\nmae_baseline <- mean(test$abs_error_baseline, na.rm = TRUE)\nmae_enhanced <- mean(test$abs_error_enhanced, na.rm = TRUE)\n\ncat(\"MAE Baseline LM:\", round(mae_baseline, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMAE Baseline LM: 0.599 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MAE Enhanced LM:\", round(mae_enhanced, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMAE Enhanced LM: 0.601 \n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(speedglm)\nlibrary(MASS)\n\nmodel2_poisson <- speedglm(\n  Trip_Count ~ \n    hour + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    center_city_high_demand + perfect_weather,\n  data = train,\n  family = poisson()\n)\n\n\ntest$hour <- factor(test$hour, levels = levels(train$hour))\ntest$dotw_simple <- factor(test$dotw_simple, levels = levels(train$dotw_simple))\n\ntest$pred_poisson <- predict(model2_poisson, newdata = test, type = \"response\")\ntest$abs_error_poisson <- abs(test$Trip_Count - test$pred_poisson)\n\nmae_poisson <- mean(test$abs_error_poisson, na.rm = FALSE)\n\ncat(\"Poisson Test MAE =\", mae_poisson, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPoisson Test MAE = NA \n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"Poisson AIC:\", AIC(model2_poisson), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPoisson AIC: 880264.2 \n```\n\n\n:::\n\n```{.r .cell-code}\npseudo_r2 <- 1 - (model2_poisson$deviance / model2_poisson$null.deviance)\ncat(\"Pseudo R²:\", round(pseudo_r2, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPseudo R²:  \n```\n\n\n:::\n\n```{.r .cell-code}\npearson_resid <- residuals(model2_poisson, type = \"pearson\")\noverdispersion <- sum(pearson_resid^2) / model2_poisson$df.residual\n\ncat(\"Overdispersion:\", round(overdispersion, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOverdispersion:  \n```\n\n\n:::\n:::\n\n\n## Part 4: Critical Reflection \n\nOperational implications:\nThe enhanced model’s MAE is still not reliable enough for Indego to use for automated rebalancing, especially during rush hours and in Center City, where demand is most volatile. Although incorporating distance to key Center City neighborhoods slightly improved predictions for outer stations, it did not meaningfully reduce error in the core. Given these limitations, the system should only be used under stable conditions—such as good weather and outside peak periods—while separate or specialized models may be needed to capture rush-hour dynamics.\n\nEquity and model limitations:\nPrediction errors are somewhat higher in higher-income, higher-White neighborhoods, likely because these stations experience the greatest fluctuations in demand. While this pattern does not imply bias against underserved areas, there is a risk that Indego could unintentionally reinforce existing disparities if it prioritizes improving accuracy only in already well-served neighborhoods. To prevent this, demand prediction should be paired with safeguards such as minimum service standards, equity-based station planning, and periodic evaluations of model performance across demographic groups. The model also struggles during atypical periods—rush hours, weather transitions, and holidays—suggesting that additional seasonal data, event indicators, or separate modeling approaches for peak periods and major events would improve accuracy.\n\n---\n\n\n",
    "supporting": [
      "Lab5_IxchelRamirez_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}